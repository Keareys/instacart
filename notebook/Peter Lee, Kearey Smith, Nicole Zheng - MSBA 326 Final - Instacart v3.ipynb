{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas import set_option\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot\n",
    "from scipy import interp\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from  sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', True)\n",
    "pd.set_option('max_colwidth', 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEAM: Set your path accordingly\n",
    "# peter\n",
    "root_path = \"Course Data/Final/\"\n",
    "\n",
    "#kearey\n",
    "#root_path = \"fill in your path\"\n",
    "\n",
    "#nicole\n",
    "#root_path = \"fill in your path\"\n",
    "\n",
    "\n",
    "aisles = read_csv(root_path + \"aisles.csv\") \n",
    "departments = read_csv(root_path + \"departments.csv\") \n",
    "order_products__prior = read_csv(root_path + \"order_products__prior.csv\")\n",
    "order_products__train = read_csv(root_path + \"order_products__train.csv\") \n",
    "orders = read_csv(root_path + \"orders.csv\") \n",
    "products = read_csv(root_path + \"products.csv\") \n",
    "sample_submission = read_csv(root_path +  \"sample_submission.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products__train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products__prior.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling and Creating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove users with 'test' eval_set. Their final orders are not labeled  with the detail.  Thus note usabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_user_id = orders[orders['eval_set'] == 'test'].user_id.unique()\n",
    "exclude_user_id = pd.DataFrame({'user_id':exclude_user_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_orders = pd.merge(orders, exclude_user_id, how='inner', on=['user_id'])\n",
    "order_products__prior = (order_products__prior[~order_products__prior.order_id.isin(exclude_orders.order_id)])\n",
    "orders = (orders[~orders.order_id.isin(exclude_orders.order_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders[orders['eval_set'] == 'test'].user_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buld our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reordered on 'train' orders\n",
    "data_yesOrderLast  = pd.merge(order_products__train, products, how='inner', on=['product_id'])\n",
    "data_yesOrderLast  = pd.merge(data_yesOrderLast, orders, how='inner', on=['order_id'])\n",
    "data_yesOrderLast = data_yesOrderLast.groupby(['user_id', 'product_id', 'order_id'])['order_number'].count().reset_index()\n",
    "data_yesOrderLast.columns = ['user_id', 'product_id', 'order_id', 'reordered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user,item reorder count (i.e how many times they ordered the item in the past)\n",
    "data_t1  = pd.merge(order_products__prior, products, how='inner', on=['product_id'])\n",
    "data_t1  = pd.merge(data_t1, orders, how='inner', on=['order_id'])\n",
    "data_t1 = data_t1.groupby(['user_id', 'product_id'])['order_id'].count().reset_index()\n",
    "data_t1.columns = ['user_id', 'product_id', 'reorder_count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user, prior order count (i.e how many times they make any order)\n",
    "data_t2 = pd.merge(order_products__prior, orders, how='inner', on=['order_id'])\n",
    "data_t2 = data_t2.groupby(['user_id'])['order_number'].max().reset_index()\n",
    "data_t2.columns = ['user_id', 'prior_ordercount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more features here based on prior data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp data\n",
    "data_t3a = pd.merge(order_products__prior, orders[orders['eval_set']=='prior'], how='inner', on=['order_id'])\n",
    "data_t3b = pd.merge(order_products__train, orders[orders['eval_set']=='train'], how='inner', on=['order_id'])\n",
    "data_t3 = pd.concat([data_t3a, data_t3b])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t3 = data_t3.sort_values(by=['user_id','product_id', 'order_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t3t =  pd.merge(data_t3, data_t2, how='inner', on=['user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t3t['orderedAtLastOne'] = data_t3t['prior_ordercount'] == data_t3t['order_number']\n",
    "data_t3t['orderedAtLastTwo'] = data_t3t['prior_ordercount']-1 == data_t3t['order_number']\n",
    "data_t3t['orderedAtLastThree'] = data_t3t['prior_ordercount']-2 == data_t3t['order_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User ordered the product at Train order - 1\n",
    "data_t4 = data_t3t.groupby(['user_id', 'product_id'])['orderedAtLastOne'].max().reset_index()\n",
    "data_t4.columns = ['user_id', 'product_id', 'orderedAtLastOne']\n",
    "data_t4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User ordered the product at Train order - 2\n",
    "data_t5 = data_t3t.groupby(['user_id', 'product_id'])['orderedAtLastTwo'].max().reset_index()\n",
    "data_t5.columns = ['user_id', 'product_id', 'orderedAtLastTwo']\n",
    "data_t5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User ordered the product at Train order - 3\n",
    "data_t6 = data_t3t.groupby(['user_id', 'product_id'])['orderedAtLastThree'].max().reset_index()\n",
    "data_t6.columns = ['user_id', 'product_id', 'orderedAtLastThree']\n",
    "data_t6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User's average day prior orders\n",
    "data_t7 = orders[orders['eval_set']=='prior'].groupby(['user_id'])['days_since_prior_order'].mean().reset_index()\n",
    "data_t7.columns = ['user_id', 'avg_days_since_prior_order']\n",
    "data_t7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average add to cart order\n",
    "data_t8 = data_t3t.groupby(['user_id', 'product_id'])['add_to_cart_order'].mean().reset_index()\n",
    "data_t8.columns = ['user_id', 'product_id', 'avg_add_to_cart_order']\n",
    "data_t8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start combining the data\n",
    "data_prior =  pd.merge(data_t1, data_t2, how='inner', on=['user_id'])\n",
    "data_prior.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prior =  pd.merge(data_prior, data_t4, how='inner', on=['user_id', 'product_id'])\n",
    "data_prior.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prior =  pd.merge(data_prior, data_t5, how='inner', on=['user_id', 'product_id'])\n",
    "data_prior.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prior =  pd.merge(data_prior, data_t6, how='inner', on=['user_id', 'product_id'])\n",
    "data_prior.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prior =  pd.merge(data_prior, data_t7, how='inner', on=['user_id'])\n",
    "data_prior.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prior =  pd.merge(data_prior, data_t8, how='inner', on=['user_id', 'product_id'])\n",
    "data_prior.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data_prior, data_yesOrderLast, how='left', on=['user_id', 'product_id'])\n",
    "data[['reordered']] = data[['reordered']].fillna(value=0)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, products, how='left', on=['product_id'])\n",
    "data = pd.merge(data, departments, how='left', on=['department_id'])\n",
    "data = pd.merge(data, aisles, how='left', on=['aisle_id'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the data (Test debug code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_data = pd.merge(order_products__prior, orders[orders['eval_set']=='prior'], how='inner', on=['order_id'])\n",
    "test_data_temp = pd.merge(order_products__train, orders[orders['eval_set']=='train'], how='inner', on=['order_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t1 = prior_data[prior_data['user_id'] == 202279].sort_values(by=['product_id', 'order_number'])\n",
    "t2 = test_data_temp[test_data_temp['user_id'] == 202279].sort_values(by=['product_id', 'order_number'])\n",
    "\n",
    "t3 = pd.concat([t1, t2])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3.sort_values(by=['product_id', 'order_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = (data[data['user_id'] == 202279]).sort_values(by=['product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['aisle_id'] = data['aisle_id'].astype('int')\n",
    "data['department_id'] = data['department_id'].astype('int')\n",
    "data['reordered'] = data['reordered'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training KFolds Set and Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and hold out. For now use a small set to quickly run the experiments.\n",
    "trainUserIDs = orders[orders['eval_set']=='prior'].user_id.unique()\n",
    "userTrain, otherData = train_test_split(trainUserIDs,test_size = 0.99,  random_state=42)\n",
    "#userTrain, otherData = train_test_split(trainUserIDs,test_size = 0.75,  random_state=42)\n",
    "userTrainDF = pd.DataFrame({'user_id':userTrain})\n",
    "\n",
    "\n",
    "trainData = pd.merge(data, userTrainDF[['user_id']], how='inner', on=['user_id'])\n",
    "\n",
    "\n",
    "userTest, notUsed = train_test_split(otherData, test_size = 0.99,  random_state=42)\n",
    "#userTest, notUsed = train_test_split(otherData, test_size = 0.7,  random_state=42)\n",
    "userTestDF = pd.DataFrame({'user_id':userTest})\n",
    "\n",
    "testData = pd.merge(data, userTestDF[['user_id']], how='inner', on=['user_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(trainData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEAM: Experiment Models\n",
    "#selectFeatures = ['product_id',  'aisle_id', 'department_id', 'prior_ordercount', 'reorder_count']\n",
    "selectFeatures = [ 'aisle_id', 'department_id', 'prior_ordercount', 'reorder_count', 'orderedAtLastOne', \\\n",
    "                    'orderedAtLastTwo', 'orderedAtLastThree', 'avg_days_since_prior_order', 'avg_add_to_cart_order']\n",
    "outputVariable = ['reordered']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trainData[selectFeatures]\n",
    "Y = trainData[outputVariable]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparision, ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEAM: Experiment Models\n",
    "models = []\n",
    "\n",
    "models.append(('Dummy', DummyClassifier(strategy='most_frequent',random_state=0)))\n",
    "#models.append(('RandomForest1', RandomForestClassifier(n_estimators=100, max_depth = 2, random_state=42)))\n",
    "#models.append(('RandomForest2', RandomForestClassifier(n_estimators=100, max_depth = 3, random_state=42)))\n",
    "#models.append(('RandomForest3', RandomForestClassifier(n_estimators=100, max_depth = 4, random_state=42)))\n",
    "#models.append(('RandomForest4', RandomForestClassifier(n_estimators=100, max_depth = 5, random_state=42)))\n",
    "#models.append(('RandomForest5', RandomForestClassifier(n_estimators=100, max_depth = 6, random_state=42)))\n",
    "models.append(('RandomForest', RandomForestClassifier(n_estimators=100, max_depth = 3, random_state=42)))\n",
    "#models.append(('RandomForest7', RandomForestClassifier(n_estimators=100, max_depth = 8, random_state=42)))\n",
    "#models.append(('RandomForest8', RandomForestClassifier(n_estimators=100, max_depth = 9, random_state=42)))\n",
    "models.append(('AdaBoost', AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=100, algorithm=\"SAMME.R\", learning_rate=.75,random_state=42)))\n",
    "models.append(('XGBoost',XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=5, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)))\n",
    "'''\n",
    "models.append(('XGBoost2',XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=4,\n",
    " min_child_weight=5, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)))\n",
    "models.append(('XGBoost3',XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=6,\n",
    " min_child_weight=5, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)))\n",
    "models.append(('XGBoost4',XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=4, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)))\n",
    "models.append(('XGBoost5',XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=4,\n",
    " min_child_weight=4, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)))\n",
    "models.append(('XGBoost6',XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=6,\n",
    " min_child_weight=4, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import check_X_y\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "#scoring = 'roc_auc'\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "    #Bug with dummy. Suggested by https://github.com/scikit-learn/scikit-learn/issues/10786\n",
    "    X_converted, y_converted = check_X_y(X=X, y=Y)\n",
    "    cv_results = cross_val_score(model, X_converted, y_converted, cv=kfold, scoring=scoring)\n",
    "\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "## boxplot algorithm comparison\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://www.kaggle.com/kanncaa1/roc-curve-with-k-fold-cv \n",
    "X_converted, y_converted = check_X_y(X=X, y=Y)\n",
    "\n",
    "mean_fpr = dict()\n",
    "mean_tpr = dict()\n",
    "mean_auc = dict()\n",
    "\n",
    "for name, model in models:\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr[name] = np.linspace(0,1,100)\n",
    "    i = 1\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "    for train,test in kfold.split(X_converted,y_converted):\n",
    "        prediction = model.fit(X_converted[train],y_converted[train]).predict_proba(X_converted[test])\n",
    "        fpr, tpr, thres = roc_curve(y_converted[test], prediction[:, 1])\n",
    "        tprs.append(interp(mean_fpr[name], fpr, tpr))\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        i= i+1\n",
    "\n",
    "    mean_tpr[name] = np.mean(tprs, axis=0)\n",
    "    mean_auc[name] = auc(mean_fpr[name], mean_tpr[name])\n",
    "    #model.fit(X_converted, y_converted)\n",
    "    if (name == 'Dummy'):\n",
    "        model.fit(X_converted, y_converted)\n",
    "    else:\n",
    "        model.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "for name, model in models:\n",
    "    ax.plot(mean_fpr[name], mean_tpr[name], linewidth=2, label = name)\n",
    "    print(\"auc \" + name + \":\" + str(mean_auc[name]))\n",
    "    \n",
    "    \n",
    "plt.title('Model Comparison on  Performance, ROC')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score on Hold out Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test = testData[selectFeatures]\n",
    "y_test = testData[outputVariable]\n",
    "\n",
    "for name, model in models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    #y_pred = (model.predict_proba(X_test)[:,1] >= 0.33).astype(bool) # set threshold as 0.3\n",
    "    \n",
    "    print(\"model name:\" + name)\n",
    "    print(\"F1 score :\" + str(f1_score(y_test, y_pred, average=\"macro\")))\n",
    "    print(\"Precision score:\" + str(precision_score(y_test, y_pred, average=\"macro\")))\n",
    "    print(\"Recall score:\" + str(recall_score(y_test, y_pred, average=\"macro\")))\n",
    "    \n",
    "    unique_elements, counts_elements = np.unique(y_pred, return_counts=True)\n",
    "    print(\"Frequency of unique values of the predicated array:\")\n",
    "    print(np.asarray((unique_elements, counts_elements)))\n",
    "    print(\"\")\n",
    "    \n",
    "    print(confusion_matrix(y_test, y_pred))  \n",
    "    print(classification_report(y_test, y_pred))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    if (name == 'XGBoost'):\n",
    "        feat_imp = pd.Series(model.get_booster().get_fscore()).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat, but focus on XGBoost and add Penality because the target class is so imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEAM: Experiment Models\n",
    "#https://stats.stackexchange.com/questions/28029/training-a-decision-tree-against-unbalanced-data\n",
    "models = []\n",
    "\n",
    "X_converted, y_converted = check_X_y(X=X, y=Y)\n",
    "\n",
    "models.append(('Dummy', DummyClassifier(strategy='most_frequent',random_state=0)))\n",
    "models.append(('XGBoost',XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=5, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)))\n",
    "\n",
    "pos_weight = sum(y_converted==0)/sum(y_converted==1)\n",
    "models.append(('XGBoostWeight',XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=5, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=pos_weight, seed=27)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://www.kaggle.com/kanncaa1/roc-curve-with-k-fold-cv \n",
    "X_converted, y_converted = check_X_y(X=X, y=Y)\n",
    "\n",
    "mean_fpr = dict()\n",
    "mean_tpr = dict()\n",
    "mean_auc = dict()\n",
    "\n",
    "for name, model in models:\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr[name] = np.linspace(0,1,100)\n",
    "    i = 1\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "    for train,test in kfold.split(X_converted,y_converted):\n",
    "        prediction = model.fit(X_converted[train],y_converted[train]).predict_proba(X_converted[test])\n",
    "        fpr, tpr, thres = roc_curve(y_converted[test], prediction[:, 1])\n",
    "        tprs.append(interp(mean_fpr[name], fpr, tpr))\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        i= i+1\n",
    "\n",
    "    mean_tpr[name] = np.mean(tprs, axis=0)\n",
    "    mean_auc[name] = auc(mean_fpr[name], mean_tpr[name])\n",
    "    #model.fit(X_converted, y_converted)\n",
    "    if (name == 'Dummy'):\n",
    "        model.fit(X_converted, y_converted)\n",
    "    else:\n",
    "        model.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "for name, model in models:\n",
    "    ax.plot(mean_fpr[name], mean_tpr[name], linewidth=2, label = name)\n",
    "    print(\"auc \" + name + \":\" + str(mean_auc[name]))\n",
    "    \n",
    "    \n",
    "plt.title('Model Comparison on  Performance, ROC')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = testData[selectFeatures]\n",
    "y_test = testData[outputVariable]\n",
    "\n",
    "for name, model in models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    #y_pred = (model.predict_proba(X_test)[:,1] >= 0.33).astype(bool) # set threshold as 0.3\n",
    "    \n",
    "    #default\n",
    "    print(\"model name:\" + name)\n",
    "    print(\"F1 score :\" + str(f1_score(y_test, y_pred, average=\"macro\")))\n",
    "    print(\"Precision score:\" + str(precision_score(y_test, y_pred, average=\"macro\")))\n",
    "    print(\"Recall score:\" + str(recall_score(y_test, y_pred, average=\"macro\")))\n",
    "    \n",
    "    unique_elements, counts_elements = np.unique(y_pred, return_counts=True)\n",
    "    print(\"Frequency of unique values of the predicated array:\")\n",
    "    print(np.asarray((unique_elements, counts_elements)))\n",
    "    print(\"\")\n",
    "    \n",
    "    print(confusion_matrix(y_test, y_pred))  \n",
    "    print(classification_report(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    if (name == 'XGBoostWeight'):\n",
    "        feat_imp = pd.Series(model.get_booster().get_fscore()).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the Recall / Threshold tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "X_test = testData[selectFeatures]\n",
    "y_test = testData[outputVariable]\n",
    "\n",
    "#thresholds_tests = [.2, .3, .4, .5, .6, .7, .8, .9]\n",
    "thresholds_tests = np.arange(0,1.05,0.05)\n",
    "\n",
    "one_recall = []\n",
    "one_precision = [];\n",
    "one_fscore = []\n",
    "\n",
    "recall_macro = []\n",
    "precision_macro = [];\n",
    "precision_fscore = []\n",
    "\n",
    "for name, model in models:\n",
    "    \n",
    "    one_recall = []\n",
    "    one_precision = [];\n",
    "    one_fscore = []\n",
    "\n",
    "    \n",
    "    recall_macro = []\n",
    "    precision_macro = [];\n",
    "    fscore_macro = []\n",
    "\n",
    "    for threshold in thresholds_tests:\n",
    "        #y_pred = model.predict(X_test)\n",
    "        y_pred = (model.predict_proba(X_test)[:,1] >= threshold).astype(bool) # set threshold as 0.3\n",
    "        precision,recall,fscore,support=precision_recall_fscore_support(y_test,y_pred,average=None)\n",
    "        \n",
    "        precisionM,recallM,fscoreM,supportM=precision_recall_fscore_support(y_test,y_pred,average='macro')\n",
    "\n",
    "        if (recall[1] < 0.01):\n",
    "            one_precision.append(1)\n",
    "        else:    \n",
    "            one_precision.append((precision[1]))\n",
    "        one_recall.append((recall[1]))\n",
    "        one_fscore.append((fscore[1]))\n",
    "\n",
    "        #print(fscore[1])\n",
    "        \n",
    "        if (recallM < 0.01):\n",
    "            precision_macro.append(1)\n",
    "        else:    \n",
    "            precision_macro.append((precisionM))\n",
    "        recall_macro.append((recallM))\n",
    "        fscore_macro.append((fscoreM))\n",
    "\n",
    "        \n",
    "        #default\n",
    "        print(\"model name:\" + name)\n",
    "        print(\"threshold:\" + str(threshold))\n",
    "        print(\"F1 score :\" + str(f1_score(y_test, y_pred, average=\"macro\")))\n",
    "        print(\"Precision score:\" + str(precision_score(y_test, y_pred, average=\"macro\")))\n",
    "        print(\"Recall score:\" + str(recall_score(y_test, y_pred, average=\"macro\")))\n",
    "        \n",
    "    \n",
    "        unique_elements, counts_elements = np.unique(y_pred, return_counts=True)\n",
    "        print(\"Frequency of unique values of the predicated array:\")\n",
    "        print(np.asarray((unique_elements, counts_elements)))\n",
    "        print(\"\")\n",
    "    \n",
    "        print(confusion_matrix(y_test, y_pred))  \n",
    "        print(classification_report(y_test, y_pred))  \n",
    "        \n",
    "    plt.rcParams['axes.labelsize'] = 14\n",
    "    plt.rcParams['xtick.labelsize'] = 12\n",
    "    plt.rcParams['ytick.labelsize'] = 12\n",
    "    fig, ax = plt.subplots()\n",
    "    #ax.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('y value')\n",
    "\n",
    "    ax.plot(thresholds_tests, one_precision, linewidth=2, label = 'precision')\n",
    "    ax.plot(thresholds_tests, one_recall, linewidth=2, label = 'recall')\n",
    "    ax.plot(thresholds_tests, one_fscore, linewidth=2, label = 'fscore')\n",
    "\n",
    "    plt.title(\"reordered=1 Precision, Recall vs Threshold\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "      \n",
    "        \n",
    "    plt.rcParams['axes.labelsize'] = 14\n",
    "    plt.rcParams['xtick.labelsize'] = 12\n",
    "    plt.rcParams['ytick.labelsize'] = 12\n",
    "    fig, ax = plt.subplots()\n",
    "    #ax.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('y value')\n",
    "\n",
    "    ax.plot(thresholds_tests, precision_macro, linewidth=2, label = 'precision')\n",
    "    ax.plot(thresholds_tests, recall_macro, linewidth=2, label = 'recall')\n",
    "    ax.plot(thresholds_tests, fscore_macro, linewidth=2, label = 'fscore')\n",
    "\n",
    "    \n",
    "    plt.title(\"Macro Precision, Recall vs Threshold\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
